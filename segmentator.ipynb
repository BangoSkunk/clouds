{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### кратко\n",
    "Задачей в соревновании была сегментация облаков на 4 класса, причем на одном снимке могли возникать несколько облаков разного класса, маски могли пересекаться.\n",
    "\n",
    "Первоначально выбиралась архитектура сегментатора, выбор был между PSPNet и Unet, с энкодером ResNet34, лучше себя показал Unet.\n",
    "После прочтения решения Heng стало понятно, что этот вопрос стоило проработать глубже. Он использовал обе архитектуры, просто с разными энкодерами.\n",
    "\n",
    "Были испробованы различные архитектуры энкодеров: ResNet18, ResNet34, ResNet50, ResNext50, EfficientNet-b0, EfficientNet-b2. Так как маски выглядели просто как пятна, оказалось, что лучше всего работали не очень глубокие архитектуры.\n",
    "\n",
    "В качетсве лоссов тестились BCE,BCE-Dice, Focal-Dice, лучше всего показал себя BCE-Dice, для каждого из лоссов был выбран свой вес, также каждый класс имел свой вес согласно частоте возникновения в датасете.\n",
    "\n",
    "Энкодером был выбран ResNet34, предобученный на Imagenet, encoder_lr = 5e-4, decoder_lr = 5e-4, scheduler = reduceOnPlateau.\n",
    "Лучший результат выбирался по positive dice за эпоху( потому что для улучшения качества предсказания использовался классификатор).\n",
    "\n",
    "В результате, как показали решения других участников, стоило пробовать InceptionNet в качестве энкодера, но по каким-то даже мне неведомым причинам я его не попробовал.\n",
    "\n",
    "В результате хорошие места заняли решения на Unet+resnet/inceptionnet blend Pspnet+resnet/inceptionnet, все сети были получены на out-of-fold-cv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from segmentation_models_pytorch import Unet, PSPNet\n",
    "import os\n",
    "import cv2\n",
    "import pdb\n",
    "import time\n",
    "import warnings\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import DataLoader, Dataset, sampler\n",
    "from matplotlib import pyplot as plt\n",
    "from albumentations import (HorizontalFlip, VerticalFlip, ShiftScaleRotate, Normalize, Resize, Compose, GaussNoise, RandomBrightness, RandomContrast)\n",
    "from albumentations.pytorch import ToTensor\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "seed = 69\n",
    "random.seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "np.random.seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RLE-Mask utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/paulorzp/rle-functions-run-lenght-encode-decode\n",
    "def mask2rle(img):\n",
    "    '''\n",
    "    img: numpy array, 1 -> mask, 0 -> background\n",
    "    Returns run length as string formated\n",
    "    '''\n",
    "    pixels= img.T.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)\n",
    "\n",
    "def make_mask(row_id, df):\n",
    "    '''Given a row index, return image_id and mask (1400, 2100, 4) from the dataframe `df`'''\n",
    "    fname = df.iloc[row_id].name\n",
    "    labels = df.iloc[row_id][:4]\n",
    "    masks = np.zeros((1400, 2100, 4), dtype=np.float32) # float32 is V.Imp\n",
    "    # 4:class 1～4 (ch:0～3)\n",
    "\n",
    "    for idx, label in enumerate(labels.values):\n",
    "        if label is not np.nan:\n",
    "            label = label.split(\" \")\n",
    "            positions = map(int, label[0::2])\n",
    "            length = map(int, label[1::2])\n",
    "            mask = np.zeros(1400 * 2100, dtype=np.uint8)\n",
    "            for pos, le in zip(positions, length):\n",
    "                mask[pos:(pos + le)] = 1\n",
    "            masks[:, :, idx] = mask.reshape(1400, 2100, order='F')\n",
    "    return fname, masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CloudDataset(Dataset):\n",
    "    def __init__(self, df, data_folder, mean, std, phase):\n",
    "        self.df = df\n",
    "        self.root = data_folder\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.phase = phase\n",
    "        self.transforms = get_transforms(phase, mean, std)\n",
    "        self.fnames = self.df.index.tolist()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id, mask = make_mask(idx, self.df)\n",
    "        image_path = os.path.join(self.root, \"train_images\",  image_id)\n",
    "        img = cv2.imread(image_path)\n",
    "        augmented = self.transforms(image=img, mask=mask)\n",
    "        img = augmented['image']\n",
    "        mask = augmented['mask'] \n",
    "        mask = mask[0].permute(2, 0, 1) \n",
    "        return img, mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.fnames)\n",
    "\n",
    "\n",
    "def get_transforms(phase, mean, std):\n",
    "    list_transforms = []\n",
    "    if phase == \"train\":\n",
    "        list_transforms.extend(\n",
    "            [   \n",
    "                \n",
    "                HorizontalFlip(p=0.5),\n",
    "                VerticalFlip(p=0.5),\n",
    "                RandomBrightness(),\n",
    "                RandomContrast(),\n",
    "                GaussNoise()\n",
    "            ]\n",
    "        )\n",
    "    list_transforms.extend(\n",
    "        [\n",
    "            Resize(512,768),\n",
    "            Normalize(mean=mean, std=std, p=1),\n",
    "            ToTensor(),\n",
    "        ]\n",
    "    )\n",
    "    list_trfms = Compose(list_transforms)\n",
    "    return list_trfms\n",
    "\n",
    "def provider(\n",
    "    data_folder,\n",
    "    df_path,\n",
    "    phase,\n",
    "    mean=None,\n",
    "    std=None,\n",
    "    batch_size=8,\n",
    "    num_workers=4,\n",
    "):\n",
    "    '''Returns dataloader for the model training'''\n",
    "    df = pd.read_csv(df_path)\n",
    "    # https://www.kaggle.com/amanooo/defect-detection-starter-u-net\n",
    "    df['Image'], df['Label'] = df['Image_Label'].str.split('_').str\n",
    "    df = df.pivot(index='Image',columns='Label',values='EncodedPixels')\n",
    "    df['defects'] = df.count(axis=1)\n",
    "    \n",
    "    train_df, val_df = train_test_split(df, test_size=0.2, stratify=df[\"defects\"], random_state=69)#\n",
    "    df = train_df if phase == \"train\" else val_df\n",
    "    image_dataset = CloudDataset(df, data_folder, mean, std, phase)\n",
    "    dataloader = DataLoader(\n",
    "        image_dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        shuffle=True,   \n",
    "    )\n",
    "\n",
    "    return dataloader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some more utility functions\n",
    "\n",
    "Dice and IoU metric implementations, metric logger for training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric(probability, truth, threshold=0.5, reduction='none'):\n",
    "    '''Calculates dice of positive and negative images seperately'''\n",
    "    '''probability and truth must be torch tensors'''\n",
    "    batch_size = len(truth)\n",
    "    with torch.no_grad():\n",
    "        probability = probability.view(batch_size, -1)\n",
    "        truth = truth.view(batch_size, -1)\n",
    "        assert(probability.shape == truth.shape)\n",
    "\n",
    "        p = (probability > threshold).float()\n",
    "        t = (truth > 0.5).float()\n",
    "\n",
    "        t_sum = t.sum(-1)\n",
    "        p_sum = p.sum(-1)\n",
    "        neg_index = torch.nonzero(t_sum == 0)\n",
    "        pos_index = torch.nonzero(t_sum >= 1)\n",
    "\n",
    "        dice_neg = (p_sum == 0).float()\n",
    "        dice_pos = 2 * (p*t).sum(-1)/((p+t).sum(-1))\n",
    "\n",
    "        dice_neg = dice_neg[neg_index]\n",
    "        dice_pos = dice_pos[pos_index]\n",
    "        dice = torch.cat([dice_pos, dice_neg])\n",
    "\n",
    "        dice_neg = np.nan_to_num(dice_neg.mean().item(), 0)\n",
    "        dice_pos = np.nan_to_num(dice_pos.mean().item(), 0)\n",
    "        dice = dice.mean().item()\n",
    "\n",
    "        num_neg = len(neg_index)\n",
    "        num_pos = len(pos_index)\n",
    "\n",
    "    return dice, dice_neg, dice_pos, num_neg, num_pos\n",
    "\n",
    "class Meter:\n",
    "    '''A meter to keep track of iou and dice scores throughout an epoch'''\n",
    "    def __init__(self, phase, epoch):\n",
    "        self.base_threshold = 0.5 # <<<<<<<<<<< here's the threshold\n",
    "        self.base_dice_scores = []\n",
    "        self.dice_neg_scores = []\n",
    "        self.dice_pos_scores = []\n",
    "\n",
    "    def update(self, targets, outputs):\n",
    "        probs = torch.sigmoid(outputs)\n",
    "        dice, dice_neg, dice_pos, _, _ = metric(probs, targets, self.base_threshold)\n",
    "        self.base_dice_scores.append(dice)\n",
    "        self.dice_pos_scores.append(dice_pos)\n",
    "        self.dice_neg_scores.append(dice_neg)\n",
    "        preds = predict(probs, self.base_threshold)\n",
    "\n",
    "    def get_metrics(self):\n",
    "        dice = np.mean(self.base_dice_scores)\n",
    "        dice_neg = np.mean(self.dice_neg_scores)\n",
    "        dice_pos = np.mean(self.dice_pos_scores)\n",
    "        dices = [dice, dice_neg, dice_pos]\n",
    "        return dices\n",
    "\n",
    "def epoch_log(phase, epoch, epoch_loss, meter, start):\n",
    "    '''logging the metrics at the end of an epoch'''\n",
    "    dices = meter.get_metrics()\n",
    "    dice, dice_neg, dice_pos = dices\n",
    "    print(\"Loss: %0.4f | dice: %0.4f | dice_neg: %0.4f | dice_pos: %0.4f\" % (epoch_loss, dice, dice_neg, dice_pos))\n",
    "    return dice, dice_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "def diceloss(input, target, eps=1e-7):\n",
    "    if type(target) is not torch.long:\n",
    "        target = target.long()\n",
    "    true_1_hot = torch.eye(2)[target.squeeze(1)].to(input.device) # B x C x H x W x 2\n",
    "    true_1_hot = true_1_hot.permute(0, 4, 1, 2, 3).float() # B x 2 x C x H x W\n",
    "    true_1_hot_f = true_1_hot[:, 0:1, ...] # B x 1 x C x H x W, falses\n",
    "    true_1_hot_s = true_1_hot[:, 1:2, ...] # B x 1 x C x H x W, true\n",
    "    true_1_hot = torch.cat([true_1_hot_s, true_1_hot_f], dim=1)\n",
    "    pos_prob = torch.sigmoid(input)\n",
    "    neg_prob = 1. - pos_prob\n",
    "    probas = torch.cat([pos_prob[:, None, ...], neg_prob[:, None, ...]], dim=1)\n",
    "    intersection = torch.sum(probas * true_1_hot, dim=(3, 4))\n",
    "    cardinality = torch.sum(probas + true_1_hot, dim=(3, 4))\n",
    "    dice_loss = (2. * intersection / (cardinality + eps)).mean(dim=(0, 1))\n",
    "    return (1 - dice_loss)\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def forward(self, input, target):\n",
    "        # Inspired by the implementation of binary_cross_entropy_with_logits\n",
    "        batch_size, num_classes = input.shape[0], input.shape[1]\n",
    "        \n",
    "        if not (target.size() == input.size()):\n",
    "            raise ValueError(\"Target size ({}) must be the same as input size ({})\".format(target.size(), input.size()))\n",
    "        \n",
    "        input, target = input.reshape((batch_size, num_classes, -1)), target.reshape((batch_size, num_classes, -1))\n",
    "        \n",
    "        max_val = (-input).clamp(min=0)\n",
    "        loss = input - input * target + max_val + ((-max_val).exp() + (-input - max_val).exp()).log()\n",
    "\n",
    "        # This formula gives us the log sigmoid of 1-p if y is 0 and of p if y is 1\n",
    "        invprobs = F.logsigmoid(-input * (target * 2 - 1))\n",
    "        loss = (invprobs * self.gamma).exp() * loss\n",
    "        return loss.mean(dim=(0, 2))\n",
    "    \n",
    "class DiceLoss(nn.Module):\n",
    "    \n",
    "    def __init__(self, eps=1e-7):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        \n",
    "    def forward(self, input, target):\n",
    "        if not (target.size() == input.size()):\n",
    "            raise ValueError(\"Target size ({}) must be the same as input size ({})\".format(target.size(), input.size()))\n",
    "        dice_loss_class = diceloss(input, target, eps=self.eps)\n",
    "        return dice_loss_class\n",
    "    \n",
    "class BCELoss(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(BCELoss, self).__init__()\n",
    "        \n",
    "    def forward(self, input, target):\n",
    "        batch_size, num_classes = input.shape[0], input.shape[1]\n",
    "        if not (target.size() == input.size()):\n",
    "            raise ValueError(\"Target size ({}) must be the same as input size ({})\".format(target.size(), input.size()))\n",
    "        \n",
    "        input, target = input.reshape((batch_size, num_classes, -1)), target.reshape((batch_size, num_classes, -1))\n",
    "        \n",
    "        loss =  F.binary_cross_entropy_with_logits(input, target, reduce=False)\n",
    "        return loss.mean(dim=(0, 2))\n",
    "    \n",
    "    \n",
    "class BCEDiceLoss(nn.Module):\n",
    "    \n",
    "    def __init__(self, weight, eps):\n",
    "        super().__init__()\n",
    "        if not isinstance(weight, torch.Tensor):\n",
    "            weight = torch.tensor(weight)\n",
    "        self.register_buffer('weight', weight)\n",
    "        self.bce = BCELoss()\n",
    "        self.dice = DiceLoss(eps=eps)\n",
    "        \n",
    "    def forward(self, input, target):\n",
    "        if input.device != self.weight.device:\n",
    "            self.weight = self.weight.to(input.device)\n",
    "        bce_loss = self.bce(input, target)\n",
    "        dice_loss = self.dice(input, target)\n",
    "        return self.weight[0] * bce_loss + self.weight[1] * dice_loss\n",
    "    \n",
    "class FocalDiceLoss(nn.Module):\n",
    "    \n",
    "    def __init__(self, weight, gamma, eps):\n",
    "        super().__init__()\n",
    "        if not isinstance(weight, torch.Tensor):\n",
    "            weight = torch.tensor(weight)\n",
    "        self.register_buffer('weight', weight)\n",
    "        self.focal = FocalLoss(gamma)\n",
    "        self.dice = DiceLoss(eps=eps)\n",
    "        \n",
    "    def forward(self, input, target):\n",
    "        focal_loss = self.focal(input, target)\n",
    "        dice_loss = self.dice(input, target)\n",
    "        return self.weight[0] * focal_loss + self.weight[1] * dice_loss\n",
    "    \n",
    "losses_type = {\n",
    "    'bce': BCELoss,\n",
    "    'dice': DiceLoss,\n",
    "    'focal': FocalLoss,\n",
    "    'bce_dice': BCEDiceLoss,\n",
    "    'focal_dice': FocalDiceLoss\n",
    "}\n",
    "\n",
    "class WeightedLoss(nn.Module):\n",
    "    \n",
    "    def __init__(self, weight, loss_type, loss_params):\n",
    "        super().__init__()\n",
    "        if not isinstance(weight, torch.Tensor):\n",
    "            weight = torch.tensor(weight)\n",
    "        self.register_buffer('weight', weight)\n",
    "        self.loss = losses_type[loss_type](**loss_params)\n",
    "        \n",
    "    def forward(self, input, target):\n",
    "        weight = self.weight\n",
    "        if input.device != weight.device:\n",
    "            weight = weight.to(input.device)\n",
    "        loss_per_class = self.loss(input, target)\n",
    "        return (weight * loss_per_class).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Unet(\"resnet34\", encoder_weights='loaded', classes=4, activation=None,attention_type='scse',center=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unet(\n",
       "  (encoder): ResNetEncoder(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (4): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (5): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  )\n",
       "  (decoder): UnetDecoder(\n",
       "    (center): CenterBlock(\n",
       "      (attention1): Identity()\n",
       "      (attention2): Identity()\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dReLU(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace)\n",
       "          )\n",
       "        )\n",
       "        (1): Conv2dReLU(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layer1): DecoderBlock(\n",
       "      (attention1): SCSEModule(\n",
       "        (cSE): Sequential(\n",
       "          (0): AdaptiveAvgPool2d(output_size=1)\n",
       "          (1): Conv2d(768, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (2): ReLU(inplace)\n",
       "          (3): Conv2d(48, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (4): Sigmoid()\n",
       "        )\n",
       "        (sSE): Sequential(\n",
       "          (0): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (attention2): SCSEModule(\n",
       "        (cSE): Sequential(\n",
       "          (0): AdaptiveAvgPool2d(output_size=1)\n",
       "          (1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (2): ReLU(inplace)\n",
       "          (3): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (4): Sigmoid()\n",
       "        )\n",
       "        (sSE): Sequential(\n",
       "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dReLU(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace)\n",
       "          )\n",
       "        )\n",
       "        (1): Conv2dReLU(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layer2): DecoderBlock(\n",
       "      (attention1): SCSEModule(\n",
       "        (cSE): Sequential(\n",
       "          (0): AdaptiveAvgPool2d(output_size=1)\n",
       "          (1): Conv2d(384, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (2): ReLU(inplace)\n",
       "          (3): Conv2d(24, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (4): Sigmoid()\n",
       "        )\n",
       "        (sSE): Sequential(\n",
       "          (0): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (attention2): SCSEModule(\n",
       "        (cSE): Sequential(\n",
       "          (0): AdaptiveAvgPool2d(output_size=1)\n",
       "          (1): Conv2d(128, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (2): ReLU(inplace)\n",
       "          (3): Conv2d(8, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (4): Sigmoid()\n",
       "        )\n",
       "        (sSE): Sequential(\n",
       "          (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dReLU(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(384, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace)\n",
       "          )\n",
       "        )\n",
       "        (1): Conv2dReLU(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layer3): DecoderBlock(\n",
       "      (attention1): SCSEModule(\n",
       "        (cSE): Sequential(\n",
       "          (0): AdaptiveAvgPool2d(output_size=1)\n",
       "          (1): Conv2d(192, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (2): ReLU(inplace)\n",
       "          (3): Conv2d(12, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (4): Sigmoid()\n",
       "        )\n",
       "        (sSE): Sequential(\n",
       "          (0): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (attention2): SCSEModule(\n",
       "        (cSE): Sequential(\n",
       "          (0): AdaptiveAvgPool2d(output_size=1)\n",
       "          (1): Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (2): ReLU(inplace)\n",
       "          (3): Conv2d(4, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (4): Sigmoid()\n",
       "        )\n",
       "        (sSE): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dReLU(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace)\n",
       "          )\n",
       "        )\n",
       "        (1): Conv2dReLU(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layer4): DecoderBlock(\n",
       "      (attention1): SCSEModule(\n",
       "        (cSE): Sequential(\n",
       "          (0): AdaptiveAvgPool2d(output_size=1)\n",
       "          (1): Conv2d(128, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (2): ReLU(inplace)\n",
       "          (3): Conv2d(8, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (4): Sigmoid()\n",
       "        )\n",
       "        (sSE): Sequential(\n",
       "          (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (attention2): SCSEModule(\n",
       "        (cSE): Sequential(\n",
       "          (0): AdaptiveAvgPool2d(output_size=1)\n",
       "          (1): Conv2d(32, 2, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (2): ReLU(inplace)\n",
       "          (3): Conv2d(2, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (4): Sigmoid()\n",
       "        )\n",
       "        (sSE): Sequential(\n",
       "          (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dReLU(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace)\n",
       "          )\n",
       "        )\n",
       "        (1): Conv2dReLU(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layer5): DecoderBlock(\n",
       "      (attention1): SCSEModule(\n",
       "        (cSE): Sequential(\n",
       "          (0): AdaptiveAvgPool2d(output_size=1)\n",
       "          (1): Conv2d(32, 2, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (2): ReLU(inplace)\n",
       "          (3): Conv2d(2, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (4): Sigmoid()\n",
       "        )\n",
       "        (sSE): Sequential(\n",
       "          (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (attention2): SCSEModule(\n",
       "        (cSE): Sequential(\n",
       "          (0): AdaptiveAvgPool2d(output_size=1)\n",
       "          (1): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (2): ReLU(inplace)\n",
       "          (3): Conv2d(1, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (4): Sigmoid()\n",
       "        )\n",
       "        (sSE): Sequential(\n",
       "          (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (1): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dReLU(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace)\n",
       "          )\n",
       "        )\n",
       "        (1): Conv2dReLU(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU(inplace)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_conv): Conv2d(16, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model # a *deeper* look"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    '''This class takes care of training and validation of our model'''\n",
    "    def __init__(self, model):\n",
    "        self.num_workers = 6\n",
    "        self.batch_size = {\"train\": 4, \"val\": 4}\n",
    "        self.accumulation_steps = 8 // self.batch_size['train']\n",
    "        self.lr = 5e-4\n",
    "        self.num_epochs = 30\n",
    "        self.best_loss = float(\"inf\")\n",
    "        self.best_dice = float(0)\n",
    "        self.phases = [\"train\", \"val\"]\n",
    "        self.device = torch.device(\"cuda:1\")\n",
    "        torch.set_default_tensor_type(\"torch.cuda.FloatTensor\")\n",
    "        self.net = model\n",
    "        self.criterion = WeightedLoss([0.25, 0.32, 0.23, 0.2], 'bce_dice',{'weight':torch.tensor([2.7,0.7], device=self.device),\n",
    "                                                                          'eps':1e-7})\n",
    "        self.optimizer = optim.Adam([\n",
    "                                        {\"params\": model.encoder.parameters(), \"lr\": 5e-4},\n",
    "                                        {\"params\": model.decoder.parameters(), \"lr\": 5e-3}\n",
    "                                    ], lr=self.lr)\n",
    "        self.scheduler = ReduceLROnPlateau(self.optimizer, mode=\"min\", patience=3, verbose=True)\n",
    "        self.net = self.net.to(self.device)\n",
    "        cudnn.benchmark = True\n",
    "        self.dataloaders = {\n",
    "            phase: provider(\n",
    "                data_folder=data_folder,\n",
    "                df_path=train_df_path,\n",
    "                phase=phase,\n",
    "                mean=(0.485, 0.456, 0.406),\n",
    "                std=(0.229, 0.224, 0.225),\n",
    "                batch_size=self.batch_size[phase],\n",
    "                num_workers=self.num_workers,\n",
    "            )\n",
    "            for phase in self.phases\n",
    "        }\n",
    "        self.losses = {phase: [] for phase in self.phases}\n",
    "        self.dice_scores = {phase: [] for phase in self.phases}\n",
    "        \n",
    "    def forward(self, images, targets):\n",
    "        images = images.to(self.device)\n",
    "        masks = targets.to(self.device)\n",
    "        outputs = self.net(images)\n",
    "        loss = self.criterion(outputs, masks)\n",
    "        return loss, outputs\n",
    "\n",
    "    def iterate(self, epoch, phase):\n",
    "        meter = Meter(phase, epoch)\n",
    "        start = time.strftime(\"%H:%M:%S\")\n",
    "        print(f\"Starting epoch: {epoch} | phase: {phase} | ⏰: {start}\")\n",
    "        batch_size = self.batch_size[phase]\n",
    "        self.net.train(phase == \"train\")\n",
    "        dataloader = self.dataloaders[phase]\n",
    "        running_loss = 0.0\n",
    "        total_batches = len(dataloader)\n",
    "#         tk0 = tqdm(dataloader, total=total_batches)\n",
    "        self.optimizer.zero_grad()\n",
    "        for itr, batch in enumerate(dataloader): # replace `dataloader` with `tk0` for tqdm\n",
    "            images, targets = batch\n",
    "            loss, outputs = self.forward(images, targets)\n",
    "            loss = loss / self.accumulation_steps\n",
    "            if phase == \"train\":\n",
    "                loss.backward()\n",
    "                if (itr + 1 ) % self.accumulation_steps == 0:\n",
    "                    self.optimizer.step()\n",
    "                    self.optimizer.zero_grad()\n",
    "            running_loss += loss.item()\n",
    "            outputs = outputs.detach().cpu()\n",
    "            meter.update(targets, outputs)\n",
    "#             tk0.set_postfix(loss=(running_loss / ((itr + 1))))\n",
    "        epoch_loss = (running_loss * self.accumulation_steps) / total_batches\n",
    "        dice, dice_pos = epoch_log(phase, epoch, epoch_loss, meter, start)\n",
    "        self.losses[phase].append(epoch_loss)\n",
    "        self.dice_scores[phase].append(dice)\n",
    "        torch.cuda.empty_cache()\n",
    "        return epoch_loss, dice\n",
    "\n",
    "    def start(self):\n",
    "        for epoch in range(self.num_epochs):\n",
    "            self.iterate(epoch, \"train\")\n",
    "            state = {\n",
    "                \"epoch\": epoch,\n",
    "                \"best_loss\": self.best_loss,\n",
    "                \"state_dict\": self.net.state_dict(),\n",
    "                \"optimizer\": self.optimizer.state_dict(),\n",
    "                'best_dice': self.best_dice\n",
    "            }\n",
    "            with torch.no_grad():\n",
    "                val_loss, dice = self.iterate(epoch, \"val\")\n",
    "                self.scheduler.step(val_loss)\n",
    "            if dice > self.best_dice:\n",
    "                print(\"******** New optimal found, saving state ********\")\n",
    "                state[\"best_dice\"] = self.best_dice = dice\n",
    "                torch.save(state, \"./model/Unet34_bcedice_enc_dec_all_pic.pth\")\n",
    "            print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission_path = '/var/home/a.kulikov/clouds/data/sample_submission.csv'\n",
    "train_df_path = '/var/home/a.kulikov/clouds/data/train.csv'\n",
    "data_folder = \"/var/home/a.kulikov/clouds/data/\"\n",
    "test_data_folder = \"/var/home/a.kulikov/clouds/data/test_images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch: 0 | phase: train | ⏰: 13:55:32\n",
      "Loss: 1.3051 | dice: 0.2752 | dice_neg: 0.0000 | dice_pos: 0.2752\n",
      "Starting epoch: 0 | phase: val | ⏰: 14:08:16\n",
      "Loss: 1.1544 | dice: 0.3950 | dice_neg: 0.0000 | dice_pos: 0.3950\n",
      "******** New optimal found, saving state ********\n",
      "\n",
      "Starting epoch: 1 | phase: train | ⏰: 14:09:58\n",
      "Loss: 1.1764 | dice: 0.4118 | dice_neg: 0.0000 | dice_pos: 0.4118\n",
      "Starting epoch: 1 | phase: val | ⏰: 14:22:38\n",
      "Loss: 1.1429 | dice: 0.4750 | dice_neg: 0.0000 | dice_pos: 0.4750\n",
      "******** New optimal found, saving state ********\n",
      "\n",
      "Starting epoch: 2 | phase: train | ⏰: 14:24:19\n",
      "Loss: 1.1432 | dice: 0.4424 | dice_neg: 0.0000 | dice_pos: 0.4424\n",
      "Starting epoch: 2 | phase: val | ⏰: 14:37:00\n",
      "Loss: 1.0805 | dice: 0.4794 | dice_neg: 0.0000 | dice_pos: 0.4794\n",
      "******** New optimal found, saving state ********\n",
      "\n",
      "Starting epoch: 3 | phase: train | ⏰: 14:38:39\n",
      "Loss: 1.1233 | dice: 0.4548 | dice_neg: 0.0000 | dice_pos: 0.4548\n",
      "Starting epoch: 3 | phase: val | ⏰: 14:51:13\n",
      "Loss: 1.0624 | dice: 0.5184 | dice_neg: 0.0000 | dice_pos: 0.5184\n",
      "******** New optimal found, saving state ********\n",
      "\n",
      "Starting epoch: 4 | phase: train | ⏰: 14:52:53\n",
      "Loss: 1.1144 | dice: 0.4606 | dice_neg: 0.0000 | dice_pos: 0.4606\n",
      "Starting epoch: 4 | phase: val | ⏰: 15:05:30\n",
      "Loss: 1.0606 | dice: 0.5100 | dice_neg: 0.0000 | dice_pos: 0.5100\n",
      "\n",
      "Starting epoch: 5 | phase: train | ⏰: 15:07:08\n",
      "Loss: 1.1015 | dice: 0.4673 | dice_neg: 0.0000 | dice_pos: 0.4673\n",
      "Starting epoch: 5 | phase: val | ⏰: 15:18:37\n",
      "Loss: 1.0367 | dice: 0.5099 | dice_neg: 0.0000 | dice_pos: 0.5099\n",
      "\n",
      "Starting epoch: 6 | phase: train | ⏰: 15:19:39\n",
      "Loss: 1.0881 | dice: 0.4795 | dice_neg: 0.0000 | dice_pos: 0.4795\n",
      "Starting epoch: 6 | phase: val | ⏰: 15:33:55\n",
      "Loss: 1.0381 | dice: 0.5253 | dice_neg: 0.0000 | dice_pos: 0.5253\n",
      "******** New optimal found, saving state ********\n",
      "\n",
      "Starting epoch: 7 | phase: train | ⏰: 15:36:07\n",
      "Loss: 1.0813 | dice: 0.4847 | dice_neg: 0.0000 | dice_pos: 0.4847\n",
      "Starting epoch: 7 | phase: val | ⏰: 15:51:12\n",
      "Loss: 1.0351 | dice: 0.5261 | dice_neg: 0.0000 | dice_pos: 0.5261\n",
      "******** New optimal found, saving state ********\n",
      "\n",
      "Starting epoch: 8 | phase: train | ⏰: 15:53:18\n",
      "Loss: 1.0754 | dice: 0.4888 | dice_neg: 0.0000 | dice_pos: 0.4888\n",
      "Starting epoch: 8 | phase: val | ⏰: 16:08:24\n",
      "Loss: 1.0395 | dice: 0.5113 | dice_neg: 0.0000 | dice_pos: 0.5113\n",
      "\n",
      "Starting epoch: 9 | phase: train | ⏰: 16:10:35\n",
      "Loss: 1.0658 | dice: 0.4970 | dice_neg: 0.0000 | dice_pos: 0.4970\n",
      "Starting epoch: 9 | phase: val | ⏰: 16:25:38\n",
      "Loss: 1.0389 | dice: 0.5152 | dice_neg: 0.0000 | dice_pos: 0.5152\n",
      "\n",
      "Starting epoch: 10 | phase: train | ⏰: 16:27:44\n",
      "Loss: 1.0580 | dice: 0.5032 | dice_neg: 0.0000 | dice_pos: 0.5032\n",
      "Starting epoch: 10 | phase: val | ⏰: 16:43:03\n",
      "Loss: 1.0452 | dice: 0.5074 | dice_neg: 0.0000 | dice_pos: 0.5074\n",
      "\n",
      "Starting epoch: 11 | phase: train | ⏰: 16:45:11\n",
      "Loss: 1.0500 | dice: 0.5123 | dice_neg: 0.0000 | dice_pos: 0.5123\n",
      "Starting epoch: 11 | phase: val | ⏰: 17:00:13\n",
      "Loss: 1.0788 | dice: 0.5034 | dice_neg: 0.0000 | dice_pos: 0.5034\n",
      "Epoch    11: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch    11: reducing learning rate of group 1 to 5.0000e-04.\n",
      "\n",
      "Starting epoch: 12 | phase: train | ⏰: 17:02:19\n",
      "Loss: 1.0192 | dice: 0.5260 | dice_neg: 0.0000 | dice_pos: 0.5260\n",
      "Starting epoch: 12 | phase: val | ⏰: 17:17:33\n",
      "Loss: 0.9915 | dice: 0.5397 | dice_neg: 0.0000 | dice_pos: 0.5397\n",
      "******** New optimal found, saving state ********\n",
      "\n",
      "Starting epoch: 13 | phase: train | ⏰: 17:19:41\n",
      "Loss: 0.9934 | dice: 0.5478 | dice_neg: 0.0000 | dice_pos: 0.5478\n",
      "Starting epoch: 13 | phase: val | ⏰: 17:34:40\n",
      "Loss: 0.9896 | dice: 0.5453 | dice_neg: 0.0000 | dice_pos: 0.5453\n",
      "******** New optimal found, saving state ********\n",
      "\n",
      "Starting epoch: 14 | phase: train | ⏰: 17:36:55\n",
      "Loss: 0.9897 | dice: 0.5531 | dice_neg: 0.0000 | dice_pos: 0.5531\n",
      "Starting epoch: 14 | phase: val | ⏰: 17:51:53\n",
      "Loss: 0.9890 | dice: 0.5395 | dice_neg: 0.0000 | dice_pos: 0.5395\n",
      "\n",
      "Starting epoch: 15 | phase: train | ⏰: 17:53:58\n",
      "Loss: 0.9797 | dice: 0.5604 | dice_neg: 0.0000 | dice_pos: 0.5604\n",
      "Starting epoch: 15 | phase: val | ⏰: 18:04:19\n",
      "Loss: 0.9959 | dice: 0.5513 | dice_neg: 0.0000 | dice_pos: 0.5513\n",
      "******** New optimal found, saving state ********\n",
      "\n",
      "Starting epoch: 16 | phase: train | ⏰: 18:05:22\n",
      "Loss: 0.9771 | dice: 0.5579 | dice_neg: 0.0000 | dice_pos: 0.5579\n",
      "Starting epoch: 16 | phase: val | ⏰: 18:15:10\n",
      "Loss: 0.9895 | dice: 0.5526 | dice_neg: 0.0000 | dice_pos: 0.5526\n",
      "******** New optimal found, saving state ********\n",
      "\n",
      "Starting epoch: 17 | phase: train | ⏰: 18:16:12\n",
      "Loss: 0.9729 | dice: 0.5654 | dice_neg: 0.0000 | dice_pos: 0.5654\n",
      "Starting epoch: 17 | phase: val | ⏰: 18:26:02\n",
      "Loss: 0.9842 | dice: 0.5398 | dice_neg: 0.0000 | dice_pos: 0.5398\n",
      "\n",
      "Starting epoch: 18 | phase: train | ⏰: 18:27:05\n",
      "Loss: 0.9664 | dice: 0.5701 | dice_neg: 0.0000 | dice_pos: 0.5701\n",
      "Starting epoch: 18 | phase: val | ⏰: 18:36:51\n",
      "Loss: 0.9968 | dice: 0.5433 | dice_neg: 0.0000 | dice_pos: 0.5433\n",
      "\n",
      "Starting epoch: 19 | phase: train | ⏰: 18:37:53\n",
      "Loss: 0.9627 | dice: 0.5707 | dice_neg: 0.0000 | dice_pos: 0.5707\n",
      "Starting epoch: 19 | phase: val | ⏰: 18:47:42\n",
      "Loss: 0.9887 | dice: 0.5489 | dice_neg: 0.0000 | dice_pos: 0.5489\n",
      "\n",
      "Starting epoch: 20 | phase: train | ⏰: 18:48:45\n",
      "Loss: 0.9575 | dice: 0.5759 | dice_neg: 0.0000 | dice_pos: 0.5759\n",
      "Starting epoch: 20 | phase: val | ⏰: 18:58:33\n",
      "Loss: 0.9912 | dice: 0.5469 | dice_neg: 0.0000 | dice_pos: 0.5469\n",
      "\n",
      "Starting epoch: 21 | phase: train | ⏰: 18:59:35\n",
      "Loss: 0.9547 | dice: 0.5786 | dice_neg: 0.0000 | dice_pos: 0.5786\n",
      "Starting epoch: 21 | phase: val | ⏰: 19:09:23\n",
      "Loss: 0.9914 | dice: 0.5415 | dice_neg: 0.0000 | dice_pos: 0.5415\n",
      "Epoch    21: reducing learning rate of group 0 to 5.0000e-06.\n",
      "Epoch    21: reducing learning rate of group 1 to 5.0000e-05.\n",
      "\n",
      "Starting epoch: 22 | phase: train | ⏰: 19:10:28\n",
      "Loss: 0.9489 | dice: 0.5803 | dice_neg: 0.0000 | dice_pos: 0.5803\n",
      "Starting epoch: 22 | phase: val | ⏰: 19:20:17\n",
      "Loss: 0.9899 | dice: 0.5498 | dice_neg: 0.0000 | dice_pos: 0.5498\n",
      "\n",
      "Starting epoch: 23 | phase: train | ⏰: 19:21:20\n",
      "Loss: 0.9390 | dice: 0.5870 | dice_neg: 0.0000 | dice_pos: 0.5870\n",
      "Starting epoch: 23 | phase: val | ⏰: 19:31:09\n",
      "Loss: 0.9848 | dice: 0.5523 | dice_neg: 0.0000 | dice_pos: 0.5523\n",
      "\n",
      "Starting epoch: 24 | phase: train | ⏰: 19:32:10\n",
      "Loss: 0.9383 | dice: 0.5887 | dice_neg: 0.0000 | dice_pos: 0.5887\n",
      "Starting epoch: 24 | phase: val | ⏰: 19:41:55\n",
      "Loss: 0.9903 | dice: 0.5544 | dice_neg: 0.0000 | dice_pos: 0.5544\n",
      "******** New optimal found, saving state ********\n",
      "\n",
      "Starting epoch: 25 | phase: train | ⏰: 19:42:57\n",
      "Loss: 0.9390 | dice: 0.5854 | dice_neg: 0.0000 | dice_pos: 0.5854\n",
      "Starting epoch: 25 | phase: val | ⏰: 19:52:43\n",
      "Loss: 0.9887 | dice: 0.5530 | dice_neg: 0.0000 | dice_pos: 0.5530\n",
      "Epoch    25: reducing learning rate of group 0 to 5.0000e-07.\n",
      "Epoch    25: reducing learning rate of group 1 to 5.0000e-06.\n",
      "\n",
      "Starting epoch: 26 | phase: train | ⏰: 19:53:45\n",
      "Loss: 0.9401 | dice: 0.5863 | dice_neg: 0.0000 | dice_pos: 0.5863\n",
      "Starting epoch: 26 | phase: val | ⏰: 20:03:31\n",
      "Loss: 0.9866 | dice: 0.5494 | dice_neg: 0.0000 | dice_pos: 0.5494\n",
      "\n",
      "Starting epoch: 27 | phase: train | ⏰: 20:04:33\n",
      "Loss: 0.9358 | dice: 0.5893 | dice_neg: 0.0000 | dice_pos: 0.5893\n",
      "Starting epoch: 27 | phase: val | ⏰: 20:14:16\n",
      "Loss: 0.9878 | dice: 0.5622 | dice_neg: 0.0000 | dice_pos: 0.5622\n",
      "******** New optimal found, saving state ********\n",
      "\n",
      "Starting epoch: 28 | phase: train | ⏰: 20:15:19\n",
      "Loss: 0.9393 | dice: 0.5866 | dice_neg: 0.0000 | dice_pos: 0.5866\n",
      "Starting epoch: 28 | phase: val | ⏰: 20:25:04\n",
      "Loss: 0.9933 | dice: 0.5458 | dice_neg: 0.0000 | dice_pos: 0.5458\n",
      "\n",
      "Starting epoch: 29 | phase: train | ⏰: 20:26:09\n",
      "Loss: 0.9388 | dice: 0.5872 | dice_neg: 0.0000 | dice_pos: 0.5872\n",
      "Starting epoch: 29 | phase: val | ⏰: 20:35:54\n",
      "Loss: 0.9890 | dice: 0.5553 | dice_neg: 0.0000 | dice_pos: 0.5553\n",
      "Epoch    29: reducing learning rate of group 0 to 5.0000e-08.\n",
      "Epoch    29: reducing learning rate of group 1 to 5.0000e-07.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#resnet_34_aug_30epochs+bce_dice_loss\n",
    "model_trainer = Trainer(model)\n",
    "model_trainer.start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 anaconda(A.Kulikov)",
   "language": "python",
   "name": "python3_akulikov"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
